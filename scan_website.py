"""
Generic Website Scanner
Scan any website for API endpoints

Usage:
    python scan_website.py https://example.com
    python scan_website.py https://example.com --pages 50 --depth 4
"""

import asyncio
import sys
import os
import json
import argparse

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from runtime.crawl import RuntimeCrawler, CrawlConfig

async def scan_website(target_url, max_pages=30, max_depth=3):
    print("=" * 60)
    print("ğŸ” JS Endpoint Scanner")
    print("=" * 60)
    print(f"ğŸ¯ Target: {target_url}")
    print(f"ğŸ“„ Max Pages: {max_pages}")
    print(f"ğŸ”— Max Depth: {max_depth}")
    print("=" * 60)
    print()
    
    # Configure crawler
    config = CrawlConfig(
        target_url=target_url,
        max_pages=max_pages,
        max_depth=max_depth,
        headless=True,
        simulate_user=False,  # HÄ±zlandÄ±rmak iÃ§in kapalÄ± (scroll/click simÃ¼lasyonu yok)
        timeout_ms=30000,  # 30 saniye (120 saniye yerine)
        wait_for_network_idle=False,
        respect_robots_txt=True,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    
    crawler = RuntimeCrawler(config)
    
    try:
        print("ğŸš€ Starting crawl...\n")
        results = await crawler.crawl()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š SCAN RESULTS")
        print("=" * 60)
        print(f"âœ“ Pages crawled: {results['statistics']['pages_crawled']}")
        print(f"âœ“ Network requests: {results['statistics']['network_requests']}")
        print(f"âœ“ Unique endpoints: {results['statistics']['unique_endpoints']}")
        print(f"âœ“ Duration: {results['statistics']['duration_ms']:.0f}ms")
        print()
        
        if results['endpoints']:
            print("=" * 60)
            print("ğŸ¯ DISCOVERED API ENDPOINTS")
            print("=" * 60)
            
            for idx, ep in enumerate(results['endpoints'][:30], 1):
                print(f"\n{idx}. {ep['method']:6} {ep['url_template']}")
                if ep.get('parameters'):
                    params = ', '.join([p['name'] for p in ep['parameters'][:5]])
                    if len(ep['parameters']) > 5:
                        params += f" ... (+{len(ep['parameters'])-5} more)"
                    print(f"   Parameters: {params}")
        
        # Generate output filename from URL
        domain = target_url.replace('https://', '').replace('http://', '').replace('/', '-').replace(':', '-')
        output_file = f'scan-{domain}.json'
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Full results saved to: {output_file}")
        print("\nâœ… Scan completed successfully!")
        
        # Return output file for AI analysis
        return output_file
        
    except Exception as e:
        print(f"\nâŒ Scan failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
        return None
        return None

def run_ai_analysis(scan_file, model="gpt-3.5-turbo"):
    """Run AI analysis on scan results"""
    api_key = os.getenv('OPENAI_API_KEY')
    
    if not api_key:
        print("\nâš ï¸  OpenAI API key bulunamadÄ± - AI analizi atlanÄ±yor")
        print("   AI analizi iÃ§in: $env:OPENAI_API_KEY=\"your-key\"")
        return
    
    print("\n" + "=" * 60)
    print("ğŸ¤– AI ANALÄ°ZÄ° BAÅLATILIYOR...")
    print("=" * 60)
    
    try:
        # analyze_with_ai.py'yi Ã§alÄ±ÅŸtÄ±r
        import subprocess
        result = subprocess.run(
            [sys.executable, 'analyze_with_ai.py', scan_file, '--model', model],
            cwd=os.path.dirname(os.path.abspath(__file__)),
            capture_output=False,
            text=True
        )
        
        if result.returncode != 0:
            print("âš ï¸  AI analizi tamamlanamadÄ±")
    except Exception as e:
        print(f"âš ï¸  AI analizi hatasÄ±: {e}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Scan website for API endpoints')
    parser.add_argument('url', help='Target URL (e.g., https://example.com)')
    parser.add_argument('--pages', type=int, default=10, help='Max pages to crawl (default: 10)')
    parser.add_argument('--depth', type=int, default=2, help='Max crawl depth (default: 2)')
    parser.add_argument('--no-ai', action='store_true', help='Skip AI analysis (only scan)')
    parser.add_argument('--ai-model', type=str, default='gpt-3.5-turbo', help='AI model (gpt-3.5-turbo or gpt-4)')
    
    args = parser.parse_args()
    
    # Validate URL
    if not args.url.startswith('http'):
        print("âŒ Error: URL must start with http:// or https://")
        sys.exit(1)
    
    # Run scan
    output_file = asyncio.run(scan_website(args.url, args.pages, args.depth))
    
    # Run AI analysis (unless --no-ai flag is set)
    if not args.no_ai and output_file:
        run_ai_analysis(output_file, args.ai_model)
